{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch.nn import Embedding, Linear, Bilinear, BatchNorm1d, ReLU, Dropout, MarginRankingLoss\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosNegData:\n",
    "\n",
    "    def __init__(self, pos_data, neg_data, weight):\n",
    "        self.pos = pos_data\n",
    "        self.neg = neg_data\n",
    "        self.weight = weight\n",
    "\n",
    "\n",
    "class Data:\n",
    "\n",
    "    def __init__(self, user_id, item_id, metadata):\n",
    "        self.user_id = torch.tensor(user_id)\n",
    "        self.item_id = torch.tensor(item_id)\n",
    "        self.metadata = torch.tensor(metadata)\n",
    "\n",
    "\n",
    "class DataGenerator(Dataset):\n",
    "\n",
    "    def __init__(self, state_history, reward_history, action_history):\n",
    "        self.state_history = state_history\n",
    "        self.reward_history = reward_history\n",
    "        self.action_history = action_history\n",
    "        self.data = []\n",
    "        self._init_pos_neg()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def _init_pos_neg(self):\n",
    "        for i, r in enumerate(self.reward_history):\n",
    "            if r > 0:\n",
    "                user_id = self.state_history[i][0][0]\n",
    "                action = self.action_history[i]\n",
    "                pos_data = Data(user_id=user_id, item_id=self.state_history[i][action][1],\n",
    "                                metadata=self.state_history[i][action][2:])\n",
    "                for j, state in enumerate(self.state_history[i]):\n",
    "                    item_id = state[1]\n",
    "                    metadata = state[2:]\n",
    "                    data = Data(user_id=user_id, item_id=item_id, metadata=metadata)\n",
    "                    if j != action:\n",
    "                        self.data.append(PosNegData(pos_data, data, 1))\n",
    "\n",
    "    def add_data(self, state, action, reward):\n",
    "        if reward > 0:\n",
    "            user_id = state[0][0]\n",
    "            pos_data = Data(user_id=user_id, item_id=state[action][1], metadata=state[action][2:])\n",
    "            for j, my_state in enumerate(state):\n",
    "                item_id = my_state[1]\n",
    "                metadata = my_state[2:]\n",
    "                data = Data(user_id=user_id, item_id=item_id, metadata=metadata)\n",
    "                if j != action:\n",
    "                    self.data.append(PosNegData(pos_data, data, 1))\n",
    "\n",
    "\n",
    "def collate_data_pos_neg(list_of_data):\n",
    "    raw_data = [data for data in list_of_data]\n",
    "    user_id_pos = torch.stack([data.pos.user_id for data in list_of_data])\n",
    "    item_id_pos = torch.stack([data.pos.item_id for data in list_of_data])\n",
    "    metadata_pos = torch.stack([data.pos.metadata for data in list_of_data])\n",
    "    user_id_neg = torch.stack([data.neg.user_id for data in list_of_data])\n",
    "    item_id_neg = torch.stack([data.neg.item_id for data in list_of_data])\n",
    "    metadata_neg = torch.stack([data.neg.metadata for data in list_of_data])\n",
    "    return {'user_id_pos': user_id_pos, 'item_id_pos': item_id_pos, 'metadata_pos': metadata_pos, 'raw_data': raw_data,\n",
    "            'user_id_neg': user_id_neg, 'item_id_neg': item_id_neg, 'metadata_neg': metadata_neg}\n",
    "\n",
    "\n",
    "def collate_data(list_of_data):\n",
    "    user_id = torch.stack([data.user_id for data in list_of_data])\n",
    "    item_id = torch.stack([data.item_id for data in list_of_data])\n",
    "    metadata = torch.stack([data.metadata for data in list_of_data])\n",
    "    return {'user_id': user_id, 'item_id': item_id, 'metadata': metadata}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interface:\n",
    "\n",
    "    def __init__(self, args):\n",
    "        self.base_url = 'http://{}'.format(args.ip_address_env_2)\n",
    "        self.user_id = args.user_id\n",
    "        self.url_reset = '{}/reset'.format(self.base_url)\n",
    "        self.url_predict = '{}/predict'.format(self.base_url)\n",
    "\n",
    "        r = requests.get(url=self.url_reset, params={'user_id': self.user_id})\n",
    "        data = r.json()\n",
    "        self.state_history = data['state_history']\n",
    "        self.rewards_history = data['rewards_history']\n",
    "        self.action_history = data['action_history']\n",
    "\n",
    "        self.nb_items = data['nb_items']\n",
    "        self.nb_users = data['nb_users']\n",
    "        self.nb_variables = len(self.state_history[0][0]) - 2\n",
    "\n",
    "        self.next_state = data['next_state']\n",
    "\n",
    "    def reset(self):\n",
    "        r = requests.get(url=self.url_reset, params={'user_id': self.user_id})\n",
    "        data = r.json()\n",
    "\n",
    "        self.state_history = data['state_history']\n",
    "        self.rewards_history = data['rewards_history']\n",
    "        self.action_history = data['action_history']\n",
    "\n",
    "        self.nb_items = data['nb_items']\n",
    "        self.nb_users = data['nb_users']\n",
    "\n",
    "        self.next_state = data['next_state']\n",
    "\n",
    "    def predict(self, recommended_item):\n",
    "        r = requests.get(url=self.url_predict, params={'user_id': self.user_id, 'recommended_item': recommended_item})\n",
    "        data = r.json()\n",
    "\n",
    "        self.state_history.append(data['state'])\n",
    "        self.rewards_history.append(data['reward'])\n",
    "        self.action_history.append(recommended_item)\n",
    "\n",
    "        self.next_state = data['state']\n",
    "        return data['state'], data['reward']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, interface):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "\n",
    "        user_embedding_dim = 10\n",
    "        item_embedding_dim = 10\n",
    "        user_meta_dim = 15\n",
    "        item_meta_dim = 15\n",
    "        meta_meta_dim = 30\n",
    "        dense_1_dim = 32\n",
    "        dense_2_dim = 15\n",
    "        out_dim = 1\n",
    "\n",
    "        self.embedding_user = Embedding(num_embeddings=interface.nb_users, embedding_dim=user_embedding_dim)\n",
    "        self.embedding_item = Embedding(num_embeddings=interface.nb_items, embedding_dim=item_embedding_dim)\n",
    "        self.concat_user_meta = Bilinear(in1_features=user_embedding_dim, in2_features=interface.nb_variables, out_features=user_meta_dim)\n",
    "        self.concat_item_meta = Bilinear(in1_features=item_embedding_dim, in2_features=interface.nb_variables, out_features=item_meta_dim)\n",
    "        self.concat_meta_meta = Bilinear(in1_features=user_meta_dim, in2_features=item_meta_dim, out_features=meta_meta_dim)\n",
    "        self.batch_norm_0 = BatchNorm1d(num_features=meta_meta_dim)\n",
    "        self.dropout_0 = Dropout(0.5)\n",
    "        self.dense_1 = Linear(in_features=meta_meta_dim, out_features=dense_1_dim)\n",
    "        self.relu_1 = ReLU()\n",
    "        self.dropout_1 = Dropout(0.5)\n",
    "        self.batch_norm_1 = BatchNorm1d(num_features=dense_1_dim)\n",
    "        self.dense_2 = Linear(in_features=dense_1_dim, out_features=dense_2_dim)\n",
    "        self.relu_2 = ReLU()\n",
    "        self.dropout_2 = Dropout(0.5)\n",
    "        self.batch_norm_2 = BatchNorm1d(num_features=dense_2_dim)\n",
    "        self.dense_3 = Linear(in_features=dense_2_dim, out_features=out_dim)\n",
    "\n",
    "    def forward(self, user_id, item_id, metadata):\n",
    "        user_embedded = self.embedding_user(user_id).squeeze(dim=1)\n",
    "        item_embedded = self.embedding_item(item_id).squeeze(dim=1)\n",
    "        user_and_meta = self.concat_user_meta(user_embedded, metadata)\n",
    "        item_and_meta = self.concat_item_meta(item_embedded, metadata)\n",
    "        meta_and_meta = self.concat_meta_meta(user_and_meta, item_and_meta)\n",
    "        output = self.batch_norm_0(meta_and_meta)\n",
    "        output = self.dropout_0(output)\n",
    "        output = self.dense_1(output)\n",
    "        output = self.relu_1(output)\n",
    "        output = self.batch_norm_1(output)\n",
    "        output = self.dropout_1(output)\n",
    "        output = self.dense_2(output)\n",
    "        output = self.relu_2(output)\n",
    "        output = self.batch_norm_2(output)\n",
    "        output = self.dropout_2(output)\n",
    "        output = self.dense_3(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, interface, learning_rate=3e-4, validation_split=0.2, batch_size=32, margin=10, min_weight=1,\n",
    "                 num_samples=100):\n",
    "        self.interface = interface\n",
    "        self.network = SiameseNetwork(interface)\n",
    "        self.dataset = DataGenerator(interface.state_history, interface.rewards_history, interface.action_history)\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_split = validation_split\n",
    "        self.min_weight = min_weight\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "        self.loss = MarginRankingLoss(margin=margin, reduction='none')\n",
    "\n",
    "        self.optimizer = Adam(self.network.parameters(), lr=learning_rate)\n",
    "        self.lr_scheduler = ReduceLROnPlateau(self.optimizer, factor=0.3, patience=5, threshold=1e-3, verbose=True)\n",
    "\n",
    "    def reset(self):\n",
    "        self.train()\n",
    "\n",
    "    def train(self):\n",
    "        weights = [data.weight for data in self.dataset]\n",
    "        sampler = WeightedRandomSampler(weights=weights, num_samples=self.num_samples, replacement=True)\n",
    "        data_loader = DataLoader(self.dataset, batch_size=self.batch_size, sampler=sampler,\n",
    "                                 collate_fn=collate_data_pos_neg, drop_last=True)\n",
    "        self.network.train()\n",
    "        cumloss = 0\n",
    "        for inputs in data_loader:\n",
    "            self.optimizer.zero_grad()\n",
    "            output_pos = self.network(inputs['user_id_pos'], inputs['item_id_pos'], inputs['metadata_pos'])\n",
    "            output_neg = self.network(inputs['user_id_neg'], inputs['item_id_neg'], inputs['metadata_neg'])\n",
    "            loss = self.loss(output_pos, output_neg, torch.ones(output_pos.shape))\n",
    "            for j, data in enumerate(inputs['raw_data']):\n",
    "                data.weight = loss[j][0].item()\n",
    "            cumloss += loss.sum().item()\n",
    "            loss = loss.mean()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        #print(cumloss / len(self.dataset))\n",
    "\n",
    "    def online(self):\n",
    "        self.network.eval()\n",
    "        l = []\n",
    "        my_state = self.interface.next_state\n",
    "        for m in self.interface.next_state:\n",
    "            data = Data(m[0], m[1], m[2:])\n",
    "            l.append(data)\n",
    "        input = collate_data(l)\n",
    "        output = self.network(input['user_id'], input['item_id'], input['metadata']).squeeze()\n",
    "        recommended_item = output.argmax().item()\n",
    "        state, reward = self.interface.predict(recommended_item)\n",
    "        self.dataset.add_data(my_state, recommended_item, reward)\n",
    "        self.train()\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 119\tTotal Reward 18550.692073\tMean Reward 155.888169   "
     ]
    }
   ],
   "source": [
    "class Argument:\n",
    "    pass\n",
    "\n",
    "args = Argument\n",
    "args.user_id = 'R3EIFXNYY6XMBXBR01BK'\n",
    "args.ip_address_env_0 = '52.47.62.31'\n",
    "args.ip_address_env_1 = '35.180.254.42'\n",
    "args.ip_address_env_2 = '35.180.178.243'\n",
    "\n",
    "interface = Interface(args)\n",
    "trainer = Trainer(interface)\n",
    "interface.reset()\n",
    "trainer.reset()\n",
    "\n",
    "reward = 0\n",
    "nb_iter = 1000\n",
    "\n",
    "for k in range(nb_iter):\n",
    "    if (k%10==9):\n",
    "        print(\"\\rIteration {}\\tTotal Reward {:5f}\\tMean Reward {:5f}   \".format(\n",
    "                    k, reward, reward/k), end=\"\")\n",
    "    # makes a prediction, returns the reward and update the network\n",
    "    reward += trainer.online()\n",
    "\n",
    "print(\"Total reward : \" + str(reward))\n",
    "print(\"Mean reward : \" + str(reward/nb_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
